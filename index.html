<!DOCTYPE html>
<!-- saved from url=(0038)https://github.com/TTxalgorithm/TTxalgorithm.github.io -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<meta name="generator" content="Hugo 0.88.1">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	 
	 
	<title>TakinAudioLLM Homepage</title>
 

  <style>
   audio, video {
       width: 100%;
       margin-bottom: 10px;
   }
   a {
	   color: #0f9c8c;
	   font-weight: bold;
   }
   .table>:not(caption)>*>* {
    	background-color: transparent;
   }
  </style>

 

<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">

<div class="container">
<header role="banner">
</header>
<main role="main">
<article itemscope="" itemtype="https://schema.org/BlogPosting">

<div class="container pt-5 mt-5 p-5 mb-5 rounded">
	<div class="text-center">
	<!-- <h2>Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models </h2> -->
	<div class="text-center:center;">
		<h2>            
            PlayMate:paper
        </h2>

      <!-- [<a href="https://arxiv.org/abs/2409.12139">Paper</a>] -->
      [<a href="">Paper</a>]
		[<a href="">Official Website</a>]
        <p class="fst-italic mb-0">
			<br>
			<b>Everest Team</b>
		</p><p></p>
        <p></p>
        <p>Ximalaya Inc.</p>
	</div>
	<p style="text-align: left"><b>Abstract:</b>
<!--		Introduce TakinAudioLLM, ....-->
We present <b>Takin-ADA</b>, which enables real-time audio-driven animation of individual portraits utilizing 3D implicit keypoints, while also allowing for precise control over facial expressions for the first time. Takin-ADA  tackles critical issues faced by existing audio-driven facial animation methods, notably expression leakage, subtle expression transfer and audio-driven precision through a two-stage approach. In the first stage, we ingeniously incorporate a canonical loss and a landmark-guided loss to enhance the transfer of subtle expressions while simultaneously mitigating expression leakage. These advancements significantly elevate the quality and realism of the generated facial animations. The second stage employs a diffusion model framework leveraging HuBERT features, which substantially improves lip-sync accuracy, ensuring a more natural and synchronized audio-visual experience. Through this two-stage approach, Takin-ADA not only generates precise lip movements but also allows flexible control over expression and head motion parameters, resulting in more natural and expressive facial animations. Takin-ADA is capable of generating high-resolution facial animations in real-time, outperforming existing commercial solutions. Extensive experiments demonstrate that our model significantly surpasses previous methods in various aspects, including video quality, facial dynamics realism, and naturalness of head movements.
	<!-- <p style="text-align: center;">
		<img src="pics/takin-tts-emo.png" height="480" width="1280">
	</p> -->
	</p>

<div class="container pt-5 mt-5 p-3 mb-5 bg-body-tertiary bg-light bg-opacity-50 rounded">
	<h2 id="overview" style="text-align: center;"></h2>

		<p style="text-align: center;">
			<img src="./src/6.png" height="512" width="512">
		</p>

	<p style="text-align: center;">
		We introduce Takin-ADA,

	</p>
</div>


 

 